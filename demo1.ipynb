{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":242954653,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, GlobalMaxPooling1D, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport polars as pl\nimport kaggle_evaluation.cmi_inference_server\n\n# Set global seed for reproducibility\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint(\"Imports loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:09.207962Z","iopub.execute_input":"2025-06-23T09:19:09.208650Z","iopub.status.idle":"2025-06-23T09:19:25.694569Z","shell.execute_reply.started":"2025-06-23T09:19:09.208626Z","shell.execute_reply":"2025-06-23T09:19:25.693833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Train Data","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv('/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv')\nprint(f\"Loaded {len(df)} rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:25.695943Z","iopub.execute_input":"2025-06-23T09:19:25.696408Z","iopub.status.idle":"2025-06-23T09:19:57.332976Z","shell.execute_reply.started":"2025-06-23T09:19:25.696386Z","shell.execute_reply":"2025-06-23T09:19:57.332269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encode gesture (our target)\n* Save encodings to file (in case we want to use model in another notebook)","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf['gesture'] = label_encoder.fit_transform(df['gesture'].astype(str))\n\n# Save class names for inference\nnp.save('gesture_classes.npy', label_encoder.classes_)\n\n# Print class label mapping\nprint(\"Gesture label mapping:\")\nfor idx, label in enumerate(label_encoder.classes_):\n    print(f\"  {idx}: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:57.333785Z","iopub.execute_input":"2025-06-23T09:19:57.334033Z","iopub.status.idle":"2025-06-23T09:19:57.435377Z","shell.execute_reply.started":"2025-06-23T09:19:57.334004Z","shell.execute_reply":"2025-06-23T09:19:57.434633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 50% of Hidden Test Set is IMU-only!\n* \"Half of the hidden-test sequences are recorded with IMU only; the thermopile (thm_) and time-of-flight (tof__v*) columns are still present but contain null values for those sequences.\"\n* https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data/data\n* Is our train data like that?  **NO!**","metadata":{}},{"cell_type":"code","source":"print(\"Checking for IMU-only sequences...\")\n\ndef check_for_imu_only_seqs():\n    # Identify thermopile and TOF columns\n    thermal_tof_cols = [col for col in df.columns if col.startswith('thm_') or col.startswith('tof_')]\n    \n    # Group by sequence and check if all thm_/tof_ values are null\n    imu_only_flags = df[thermal_tof_cols].isna().groupby(df['sequence_id']).all().all(axis=1)\n    \n    # Report statistics\n    total_sequences = df['sequence_id'].nunique()\n    imu_only_count = imu_only_flags.sum()\n    imu_only_pct = (imu_only_count / total_sequences) * 100\n    \n    print(f\"Total sequences: {total_sequences}\")\n    print(f\"IMU-only sequences (all thm_/tof_ null): {imu_only_count} ({imu_only_pct:.1f}%)\")\n\ncheck_for_imu_only_seqs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:57.436092Z","iopub.execute_input":"2025-06-23T09:19:57.436292Z","iopub.status.idle":"2025-06-23T09:19:58.962470Z","shell.execute_reply.started":"2025-06-23T09:19:57.436276Z","shell.execute_reply":"2025-06-23T09:19:58.961820Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Feature Columns (IMU-only)\n* Exclude train-only data","metadata":{}},{"cell_type":"code","source":"excluded_cols = {\n    'gesture', 'sequence_type', 'behavior', 'orientation',  # train-only\n    'row_id', 'subject', 'phase',  # metadata\n    'sequence_id', 'sequence_counter'  # identifiers\n}\n\nfeature_cols = [c for c in df.columns if c not in excluded_cols]\nimu_cols = [c for c in feature_cols if not (c.startswith(\"thm_\") or c.startswith(\"tof_\"))]\ntof_cols = [c for c in feature_cols if c.startswith(\"thm_\") or c.startswith(\"tof_\")]\n\n# Setting this true makes model ignore thermal and tof data\ndrop_thermal_and_tof = False\n\nif drop_thermal_and_tof:\n    thermal_tof_cols = [col for col in df.columns if col.startswith('thm_') or col.startswith('tof_')]\n    excluded_cols.update(thermal_tof_cols)\n    print(f\"Ignoring {len(thermal_tof_cols)} thermopile / time-of-flight columns.\")\n\n# Select numeric feature columns\n# imu_cols = [col for col in df.columns if col not in excluded_cols]\nprint(f\"Using {len(imu_cols)} numeric feature columns for training:\")\nprint(imu_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:58.964350Z","iopub.execute_input":"2025-06-23T09:19:58.964782Z","iopub.status.idle":"2025-06-23T09:19:58.970860Z","shell.execute_reply.started":"2025-06-23T09:19:58.964763Z","shell.execute_reply":"2025-06-23T09:19:58.970232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check for Missing Values\n* Yup - there are a bunch - we'll take care of these...","metadata":{}},{"cell_type":"code","source":"# Check for NaNs in selected feature columns\nnan_counts = df[imu_cols].isna().sum()\ntotal_nans = nan_counts.sum()\nprint(f\"\\nTotal missing values in feature columns: {total_nans}\")\nif total_nans > 0:\n    print(\"Columns with missing values:\")\n    print(nan_counts[nan_counts > 0])\nelse:\n    print(\"No missing values found in feature columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:58.971585Z","iopub.execute_input":"2025-06-23T09:19:58.971929Z","iopub.status.idle":"2025-06-23T09:19:59.010064Z","shell.execute_reply.started":"2025-06-23T09:19:58.971903Z","shell.execute_reply":"2025-06-23T09:19:59.009411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-process data: scale and simple impute\n* Fill missing values: forward-fill, then back-fill, then fill remaining with 0\n* Scale features to zero mean and unit variance\n* We call this function as part of building input sequences and doing inference","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\n\ndef preprocess_sequence(df_sequence: pd.DataFrame, imu_cols: list) -> np.ndarray:\n    # 1. 取原始特徵與處理缺值\n    data = df_sequence[imu_cols].copy().ffill().bfill().fillna(0)\n    \n    # 2. 標準化\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(data)\n\n    # 3. 原始統計特徵\n    mean = np.mean(scaled, axis=1, keepdims=True)\n    std = np.std(scaled, axis=1, keepdims=True)\n    max_val = np.max(scaled, axis=1, keepdims=True)\n    min_val = np.min(scaled, axis=1, keepdims=True)\n\n    # 4. 特徵強化：差分特徵\n    delta = np.diff(scaled, axis=0, prepend=scaled[[0]])\n\n    # 5. 特徵強化：移動平均\n    def moving_average(arr, window=3):\n        ret = np.cumsum(arr, axis=0)\n        ret[window:] = ret[window:] - ret[:-window]\n        return np.vstack([arr[:window-1], ret[window-1:] / window])\n    mov_avg = moving_average(scaled, window=3)\n\n    # 6. 特徵強化：range/std\n    range_val = max_val - min_val\n    range_std_ratio = range_val / (std + 1e-6)\n\n    # 7. 合併所有特徵\n    features = np.concatenate([\n        scaled,          # 原始標準化特徵\n        mean, std, max_val, min_val,  # 統計量\n        delta,           # 差分\n        mov_avg,         # 移動平均\n        range_std_ratio  # 變異程度\n    ], axis=1)\n\n    return features  # shape: (T, original_dim + 強化特徵數)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:59.010799Z","iopub.execute_input":"2025-06-23T09:19:59.011050Z","iopub.status.idle":"2025-06-23T09:19:59.017899Z","shell.execute_reply.started":"2025-06-23T09:19:59.011028Z","shell.execute_reply":"2025-06-23T09:19:59.017296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build input sequences\n* All sequences must be the same length to train the model\n* Pad or truncate to a length that covers 90% of sequence lengths\n* This defines the fixed input size for our 1D CNN","metadata":{}},{"cell_type":"code","source":"# Build sequences\nsequence_ids = df['sequence_id'].unique()\nsequences = df.groupby('sequence_id')\n\nX = []\n# X_2 = []\nseq_lengths = []\n\nprint(\"Building sequences...\")\nfor i, (seq_id, seq) in enumerate(sequences):\n    if i % 500 == 0:\n        print(f\"Processing sequence {i}...\")\n    processed = preprocess_sequence(seq, imu_cols)\n    # processed_2 = preprocess_sequence(seq, tof_cols)\n    X.append(processed)\n    # X_2.append(processed_2)\n    seq_lengths.append(processed.shape[0])\n\nmax_len_perentile = 90\n\n# Report sequence length stats\nminlen = min(seq_lengths)\navglen = int(np.mean(seq_lengths))\npad_len_to_use = int(np.percentile(seq_lengths, max_len_perentile))  \nprint(f\"Sequence length stats - Min: {minlen}, Avg: {avglen}, {max_len_perentile}th percentile: {pad_len_to_use}\")\nprint(f\"Padding / truncating all sequences to fixed length {pad_len_to_use}...\")\n\nnp.save(\"sequence_maxlen.npy\", pad_len_to_use)  # Save for inference\n\n# Pad/truncate to fixed length\nX = pad_sequences(X, maxlen=pad_len_to_use, dtype='float32', padding='post', truncating='post')\n# X_2 = pad_sequences(X_2, maxlen=pad_len_to_use, dtype='float32', padding='post', truncating='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:19:59.018533Z","iopub.execute_input":"2025-06-23T09:19:59.018774Z","iopub.status.idle":"2025-06-23T09:20:19.208534Z","shell.execute_reply.started":"2025-06-23T09:19:59.018750Z","shell.execute_reply":"2025-06-23T09:20:19.207714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare target labels as one-hot vectors\n* Use the first gesture label from each sequence as its target class (already converted to integer earlier)\n* Convert integer class labels to one-hot vectors for training (required for categorical cross-entropy)\n* The number of unique labels defines the model's output layer size","metadata":{}},{"cell_type":"code","source":"# Use groupby to get the first gesture per sequence (already integer-encoded)\ny = df.groupby('sequence_id')['gesture'].first().values\n\nprint(\"Integer labels:\", y[:4])\n\n# Convert to one-hot vectors\nnum_classes = len(np.unique(y))\ny = to_categorical(y, num_classes=num_classes)\n\nprint(\"After one-hot encoding:\", y[:4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:20:19.209647Z","iopub.execute_input":"2025-06-23T09:20:19.209921Z","iopub.status.idle":"2025-06-23T09:20:19.252093Z","shell.execute_reply.started":"2025-06-23T09:20:19.209896Z","shell.execute_reply":"2025-06-23T09:20:19.251498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train / test split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:20:19.252732Z","iopub.execute_input":"2025-06-23T09:20:19.252910Z","iopub.status.idle":"2025-06-23T09:20:19.279347Z","shell.execute_reply.started":"2025-06-23T09:20:19.252896Z","shell.execute_reply":"2025-06-23T09:20:19.278794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build, compile, and train 1D CNN model\n* Use Conv1D layers to extract temporal patterns from sensor sequences\n* Each Conv1D layer uses:\n  - `kernel_size` to define the number of time steps it looks at (temporal window)\n  - `filters` to define how many distinct patterns it tries to learn at each layer\n* Apply max pooling and dropout for regularization and dimensionality reduction\n* Flatten and pass through dense layers for final classification\n* Save trained model to disk for inference","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, MaxPooling1D, Dropout, Dense,\n    GlobalMaxPooling1D, BatchNormalization, LayerNormalization,\n    MultiHeadAttention, Add, Concatenate\n)\nfrom tensorflow.keras.models import Model\n\ndef build_transformer_multiwindow_cnn_model(input_shape, num_classes):\n    inputs = Input(shape=input_shape)  # (sequence_len, features)\n\n    # --- Transformer Block ---\n    attn_output = MultiHeadAttention(num_heads=4, key_dim=input_shape[-1])(inputs, inputs)\n    attn_output = Dropout(0.1)(attn_output)\n    out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attn_output]))\n\n    ffn = tf.keras.Sequential([\n        Dense(256, activation=\"relu\"),\n        Dense(input_shape[-1])  # residual match\n    ])\n    ffn_output = ffn(out1)\n    ffn_output = Dropout(0.1)(ffn_output)\n    x = LayerNormalization(epsilon=1e-6)(Add()([out1, ffn_output]))\n\n    # --- Multi-Window CNN Block ---\n    convs = []\n    kernel_sizes = [2, 4, 6, 8, 10]  # four parallel CNNs with different window sizes\n    for k in kernel_sizes:\n        conv = Conv1D(filters=256, kernel_size=k, padding='same', activation='relu')(x)\n        conv = BatchNormalization()(conv)\n        conv = MaxPooling1D(pool_size=2)(conv)\n        conv = Dropout(0.3)(conv)\n        convs.append(conv)\n\n    # Concatenate multi-window outputs\n    x = Concatenate()(convs)\n\n    # Refinement Conv layer (1x1 conv)\n    x = Conv1D(512, kernel_size=1, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    # --- Dense Layers ---\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs, outputs)\n    return model\n\n# Usage\ninput_shape = X_train.shape[1:]\nnum_classes = y_train.shape[-1]\nmodel = build_transformer_multiwindow_cnn_model(input_shape=input_shape, num_classes=num_classes)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n\n# Training\nmodel.fit(\n    X_train, y_train,\n    epochs=100,\n    batch_size=64,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping, reduce_lr]\n)\n\nmodel.save(\"transformer_multiwindow_cnn_model.h5\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:20:19.280066Z","iopub.execute_input":"2025-06-23T09:20:19.280312Z","iopub.status.idle":"2025-06-23T09:22:01.120570Z","shell.execute_reply.started":"2025-06-23T09:20:19.280287Z","shell.execute_reply":"2025-06-23T09:22:01.119702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Use Competition Metric for LB Estimate\n* We import: https://www.kaggle.com/code/richolson/cmi-2025-metric-copy-for-import\n* Which is a copy of: https://www.kaggle.com/code/metric/cmi-2025","metadata":{}},{"cell_type":"code","source":"from cmi_2025_metric_copy_for_import import CompetitionMetric\n\n# Get predicted labels for the validation set\nprint(\"Predicting on validation set...\")\ny_val_pred_probs = model.predict(X_val, verbose=0)\ny_val_pred = np.argmax(y_val_pred_probs, axis=1)\ny_val_true = np.argmax(y_val, axis=1)\n\n# Map integer labels back to gesture strings\ngesture_classes = np.load(\"gesture_classes.npy\", allow_pickle=True)\nval_pred_labels = pd.Series(y_val_pred).map(lambda i: gesture_classes[i])\nval_true_labels = pd.Series(y_val_true).map(lambda i: gesture_classes[i])\n\n# Build DataFrames for the metric\nval_submission = pd.DataFrame({'gesture': val_pred_labels})\nval_solution = pd.DataFrame({'gesture': val_true_labels})\n\n# Run competition metric\nmetric = CompetitionMetric()\nscore = metric.calculate_hierarchical_f1(val_solution, val_submission)\nprint(f\"Estimated leaderboard (val) score: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:22:01.121558Z","iopub.execute_input":"2025-06-23T09:22:01.121836Z","iopub.status.idle":"2025-06-23T09:22:04.416311Z","shell.execute_reply.started":"2025-06-23T09:22:01.121813Z","shell.execute_reply":"2025-06-23T09:22:04.415510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict function for inference server\n* Runs same preprocess_sequence function on inference data as we did for training\n* Pads / truncates to sequences to same length as for training\n* Loads trained model and predicts gesture class\n* Maps predicted index back to original gesture label","metadata":{}},{"cell_type":"code","source":"model = load_model(\"transformer_multiwindow_cnn_model.h5\")\ndef predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    df_seq = sequence.to_pandas()\n    processed = preprocess_sequence(df_seq, imu_cols)\n    maxlen = int(np.load(\"sequence_maxlen.npy\"))  # ensure consistent shape\n    padded = pad_sequences([processed], maxlen=maxlen, dtype='float32', padding='post', truncating='post')\n    prediction = model.predict(padded, verbose=0)\n    predicted_index = np.argmax(prediction, axis=1)[0]\n    gesture_classes = np.load(\"gesture_classes.npy\", allow_pickle=True)\n    return gesture_classes[predicted_index]\n\n# Launch inference server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:22:04.417355Z","iopub.execute_input":"2025-06-23T09:22:04.417627Z","iopub.status.idle":"2025-06-23T09:22:07.596396Z","shell.execute_reply.started":"2025-06-23T09:22:04.417607Z","shell.execute_reply":"2025-06-23T09:22:07.595657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verify prediction function","metadata":{}},{"cell_type":"code","source":"# Manual test (only runs outside Kaggle gateway)\nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"\\nRunning manual test...\")\n    test_df = pd.read_csv('/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv')\n    sample_seq_id = test_df['sequence_id'].unique()[0]\n    test_seq = test_df[test_df['sequence_id'] == sample_seq_id]\n    prediction = predict(pl.DataFrame(test_seq), None)\n    print(f\"Manual prediction result for sequence_id {sample_seq_id}: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:22:07.598788Z","iopub.execute_input":"2025-06-23T09:22:07.599046Z","iopub.status.idle":"2025-06-23T09:22:08.107943Z","shell.execute_reply.started":"2025-06-23T09:22:07.599029Z","shell.execute_reply":"2025-06-23T09:22:08.107094Z"}},"outputs":[],"execution_count":null}]}